{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a647f5e-43ac-49ae-9428-88eb8d9f3590",
   "metadata": {},
   "source": [
    "# This script extracts words from the csv \n",
    "## to be used to calculate: \n",
    "- words per minute\n",
    "- total words\n",
    "- variance of words across match \n",
    "- variance of words across speakers\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d37e157a-25ec-4ef9-8b72-5841b9a48800",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_cleaned_words_list(transcript):\n",
    "    import pandas as pd\n",
    "\n",
    "    # import transcript of interest\n",
    "    coded_transcripts_df = pd.read_csv(transcript)\n",
    "\n",
    "    # locate the columns for sentences and players\n",
    "    all_sentences_column = coded_transcripts_df.iloc[:, 3]\n",
    "    players_column = coded_transcripts_df.iloc[:, 0]\n",
    "\n",
    "    # extract the content of each column into a list\n",
    "    all_sentences = list(all_sentences_column.values) \n",
    "    players = list(players_column.values)\n",
    "    \n",
    "    # create an empty list to hold all words\n",
    "    all_words = []\n",
    "\n",
    "    # got through each sentence \n",
    "    for sentence in all_sentences:\n",
    "        split_the_sentence = sentence.split()\n",
    "\n",
    "        # from each sentence, extract the word\n",
    "        for word in split_the_sentence:\n",
    "            all_words.append(word)\n",
    "\n",
    "    # take out all the '(inaudible)' lines\n",
    "\n",
    "    cleaned_word_list = []\n",
    "\n",
    "    list_with_inaudible_words = [\"(inaudible)\", \"(inaduible\", \"cross\", \"talk)\", \"like\"]\n",
    "    for word in all_words: \n",
    "        if word in list_with_inaudible_words :\n",
    "            continue\n",
    "        else: \n",
    "            cleaned_word_list.append(word)\n",
    "\n",
    "    #print(cleaned_word_list)\n",
    "    \n",
    "    return(cleaned_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23786ecf-10de-4683-8121-2cc7d75f7687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>team_number</th>\n",
       "      <th>team_size</th>\n",
       "      <th>match_id</th>\n",
       "      <th>outcome</th>\n",
       "      <th>match_duration</th>\n",
       "      <th>kills_per_minute</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.056494e+09</td>\n",
       "      <td>loss</td>\n",
       "      <td>25:15:00</td>\n",
       "      <td>0.713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.064844e+09</td>\n",
       "      <td>won</td>\n",
       "      <td>29:39:00</td>\n",
       "      <td>1.484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.081078e+09</td>\n",
       "      <td>won</td>\n",
       "      <td>26:40:00</td>\n",
       "      <td>0.862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.060243e+09</td>\n",
       "      <td>won</td>\n",
       "      <td>23:08:00</td>\n",
       "      <td>1.470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.062655e+09</td>\n",
       "      <td>loss</td>\n",
       "      <td>33:04:00</td>\n",
       "      <td>0.877</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   team_number  team_size      match_id outcome match_duration  \\\n",
       "0          1.0        3.0  5.056494e+09    loss       25:15:00   \n",
       "1          2.0        5.0  5.064844e+09     won       29:39:00   \n",
       "2          3.0        3.0  5.081078e+09     won       26:40:00   \n",
       "3          4.0        5.0  5.060243e+09     won       23:08:00   \n",
       "4          5.0        5.0  5.062655e+09    loss       33:04:00   \n",
       "\n",
       "   kills_per_minute  \n",
       "0             0.713  \n",
       "1             1.484  \n",
       "2             0.862  \n",
       "3             1.470  \n",
       "4             0.877  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "log_of_team_data_df = pd.read_csv(r'D:\\Projects\\UG-league-project\\data\\log-of-team-data.csv')\n",
    "log_of_team_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8689920a-7fd4-4d60-bfb9-86fd97a7db35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_team_number(transcript):\n",
    "    # extract the team number\n",
    "    split_file_name_into_content = transcript.split(\"\\\\\")\n",
    "    team_number = split_file_name_into_content[5][:-4].split(\"[\")[1][:-1]\n",
    "    \n",
    "    print(\"team number = \", team_number)\n",
    "    \n",
    "    return(team_number)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7fd786f9-be1e-4b59-8777-ec7680e9db18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "team number =  16\n",
      "total words =  3002\n"
     ]
    }
   ],
   "source": [
    "cleaned_words = get_cleaned_words_list(r'D:\\Projects\\UG-league-project\\data\\coded-transcripts\\team-[16].csv')\n",
    "team_number = get_team_number(r'D:\\Projects\\UG-league-project\\data\\coded-transcripts\\team-[16].csv')\n",
    "print('total words = ', len(cleaned_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "def96269-a6c4-4d56-a620-d5a3ef893d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{16: 3002, 18: 201}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# create a dictionary where key = team number and value = total words\n",
    "# then match dictionary key to value in team number \n",
    "team_num_and_total_words_dict = {}\n",
    "team_num_and_total_words_dict[16] = 3002\n",
    "team_num_and_total_words_dict[18] = 201\n",
    "print(team_num_and_total_words_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "179de189-dded-48ad-8ae6-9bec81d0c6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "team_num_and_total_words_dict = {'Team Number' : [16, 18], 'Total Words' : [3002, 201]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3bfbbd15-344c-4e2f-9c8b-6efba31f0d2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Team Number</th>\n",
       "      <th>Total Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>3002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Team Number  Total Words\n",
       "0           16         3002\n",
       "1           18          201"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "team_num_and_total_words_df = pd.DataFrame.from_dict(team_num_and_total_words_dict)\n",
    "team_num_and_total_words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f61841c-1a99-4d53-be21-22e33d6d85fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5f3142-4b07-46e7-8d84-951d35cfbc8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# combine all the words into one long string\n",
    "full_combined_transcript = \" \".join(cleaned_word_list)\n",
    "full_combined_transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdd8513-a5f5-4931-a355-5df2840d60fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c5f643-05ad-412c-b593-a474ffcf034b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "a769356a-e558-4d1a-a69c-e52c203b9403",
   "metadata": {},
   "source": [
    "######### Cells in this section are to remind you of the steps you took to create the function ##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d682b696-bbc2-4278-ae20-f27d3c00aa6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import transcript of interest\n",
    "coded_transcripts_df = pd.read_csv(r'D:\\Projects\\UG-league-project\\data\\coded-transcripts\\team-[46]-sentiment-mid-cohesion.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3abf07-9cff-4710-87d0-a587cb8df27f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create an empty list to hold all words\n",
    "all_words = []\n",
    "\n",
    "# got through each sentence \n",
    "for sentence in all_sentences:\n",
    "    split_the_sentence = sentence.split()\n",
    "    \n",
    "    # from each sentence, extract the word\n",
    "    for word in split_the_sentence:\n",
    "        all_words.append(word)\n",
    "\n",
    "print(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab09af95-46a9-4107-9274-3805ad70ec59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# take out all the '(inaudible)' lines\n",
    "\n",
    "cleaned_word_list = []\n",
    "\n",
    "list_with_inaudible_words = [\"(inaudible)\", \"(inaduible\", \"cross\", \"talk)\", \"like\"]\n",
    "for word in all_words: \n",
    "    if word in list_with_inaudible_words :\n",
    "        continue\n",
    "    else: \n",
    "        cleaned_word_list.append(word)\n",
    "\n",
    "        \n",
    "print(cleaned_word_list)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2138b78a-9982-4968-8517-9558772ed1e4",
   "metadata": {},
   "source": [
    "######### Cells in this section are to remind you of the steps you took to create the function ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595fd57a-8acd-4441-86db-6f2be0605c1d",
   "metadata": {},
   "source": [
    "# Cleaning the transcript using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15796ab8-7d54-4bdf-9e19-0867c84ff392",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Evelyn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Evelyn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trying some NLTK stuff here\n",
    "import nltk\n",
    "nltk.download(['averaged_perceptron_tagger', \n",
    "               'stopwords'])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a1f51f63-e9ca-450b-957e-86f2422628cd",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "# split sentence into words/utterances\n",
    "tokens = nltk.word_tokenize(full_combined_transcript)\n",
    "\n",
    "#tokens\n",
    "\n",
    "# include only the words that are made up of letters\n",
    "# remove punctuation marks and numbers\n",
    "words_only_list = []\n",
    "for token in tokens: \n",
    "    if token.isalpha():\n",
    "        words_only_list.append(token)\n",
    "        \n",
    "\n",
    "print(\"total words: \", len(words_only_list))\n",
    "words_only_list\n",
    "#tagged = nltk.pos_tag(tokens)\n",
    "#tagged"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a12fa92b-7217-441c-9549-a580e26fb831",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "#from nltk.corpus import stopwords\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "filtered_list = []\n",
    "for word in words_only_list:\n",
    "    if word.casefold() not in stop_words: \n",
    "        filtered_list.append(word)\n",
    "        \n",
    "print(\"total words after removing stopwords: \", len(filtered_list))        \n",
    "filtered_list        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60acdf03-5fdd-4f5f-8b41-c17a0528742a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words after removing stopwords:  583\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Basically',\n",
       " 'proceed',\n",
       " 'nothing.',\n",
       " 'believe.',\n",
       " 'Well.',\n",
       " 'Wait',\n",
       " 'think',\n",
       " 'double',\n",
       " 'click',\n",
       " 'ward',\n",
       " 'get',\n",
       " 'one',\n",
       " 'Yea',\n",
       " 'works',\n",
       " 'buy',\n",
       " 'swaps',\n",
       " 'one',\n",
       " 'reasons',\n",
       " 'actually',\n",
       " 'never',\n",
       " 'happened',\n",
       " 'Surprised',\n",
       " 'try',\n",
       " 'level',\n",
       " 'one',\n",
       " 'cheese',\n",
       " 'honest.',\n",
       " 'Ah',\n",
       " 'cause',\n",
       " 'leashed.',\n",
       " 'Yep.',\n",
       " 'hate',\n",
       " 'see.',\n",
       " 'Yeah,',\n",
       " 'yeah,',\n",
       " 'dashes',\n",
       " 'super',\n",
       " 'compatible.',\n",
       " 'Guardian',\n",
       " 'now.',\n",
       " 'stay',\n",
       " 'bit',\n",
       " 'cause',\n",
       " \"they're\",\n",
       " 'shoving',\n",
       " 'really',\n",
       " 'enough',\n",
       " 'money',\n",
       " 'anything.',\n",
       " 'Yeah',\n",
       " \"that's\",\n",
       " 'unfortunate.',\n",
       " 'blind?',\n",
       " 'really',\n",
       " 'see',\n",
       " 'Poppy',\n",
       " 'or?',\n",
       " 'Ah',\n",
       " 'hid',\n",
       " 'last',\n",
       " 'second',\n",
       " \"there's\",\n",
       " 'much',\n",
       " 'could',\n",
       " 'done',\n",
       " 'there.',\n",
       " 'Oh',\n",
       " 'Gragas',\n",
       " 'Please',\n",
       " 'go',\n",
       " 'in.',\n",
       " 'Oh',\n",
       " 'least',\n",
       " 'Rakan',\n",
       " 'got',\n",
       " 'both.',\n",
       " 'still',\n",
       " 'there?',\n",
       " 'Fuck',\n",
       " 'oh',\n",
       " 'god.',\n",
       " 'cancelled',\n",
       " 'two',\n",
       " 'autos.',\n",
       " 'Oh',\n",
       " 'sweet',\n",
       " 'got',\n",
       " 'two',\n",
       " 'kills',\n",
       " \"I'm\",\n",
       " 'back',\n",
       " 'game.',\n",
       " 'Alright',\n",
       " 'Rakan',\n",
       " 'still',\n",
       " 'level',\n",
       " 'four',\n",
       " \"i'm\",\n",
       " 'halfway',\n",
       " 'six.',\n",
       " 'CS',\n",
       " 'lead',\n",
       " 'gain',\n",
       " 'better.',\n",
       " 'kinda',\n",
       " 'wave',\n",
       " 'Hard',\n",
       " 'shove',\n",
       " 'turret',\n",
       " 'shove',\n",
       " 'and-',\n",
       " 'Yeah.',\n",
       " 'anything',\n",
       " 'buy?',\n",
       " 'Uhh',\n",
       " 'get',\n",
       " 'Noonquiver',\n",
       " 'fuck',\n",
       " 'might',\n",
       " 'well.',\n",
       " 'Might',\n",
       " 'bit',\n",
       " 'late',\n",
       " 'cannon',\n",
       " 'wave',\n",
       " 'cannon',\n",
       " 'wave',\n",
       " 'cannon',\n",
       " 'wave.',\n",
       " \"They're\",\n",
       " 'gonna',\n",
       " 'get',\n",
       " 'Dragon.',\n",
       " \"Can't\",\n",
       " 'really',\n",
       " 'contest',\n",
       " 'mean',\n",
       " 'jungler',\n",
       " 'top',\n",
       " 'lane',\n",
       " 'anways',\n",
       " 'Yeah',\n",
       " 'zero',\n",
       " 'mid',\n",
       " 'prio',\n",
       " 'Okay',\n",
       " 'maybe',\n",
       " 'pinged',\n",
       " 'Rakan',\n",
       " 'missing',\n",
       " 'mastery',\n",
       " 'flash.',\n",
       " 'Oh',\n",
       " 'yes',\n",
       " 'mastery',\n",
       " 'flash',\n",
       " 'kill',\n",
       " 'it.',\n",
       " 'bounded',\n",
       " 'ulty.',\n",
       " 'Nice',\n",
       " 'Yeah',\n",
       " 'gets',\n",
       " 'really',\n",
       " 'annoying',\n",
       " 'play',\n",
       " 'Nidalee.',\n",
       " 'Oh',\n",
       " 'yeah.',\n",
       " 'Uhm',\n",
       " 'guys',\n",
       " 'wanna',\n",
       " 'swap',\n",
       " 'point?',\n",
       " 'Uhm',\n",
       " 'possibly',\n",
       " 'actually',\n",
       " \"I'm\",\n",
       " 'CS-ing',\n",
       " 'horribly',\n",
       " 'anyway',\n",
       " 'Oh',\n",
       " 'wait',\n",
       " 'wait.',\n",
       " 'Ahhh.',\n",
       " 'Bruhhh',\n",
       " 'want',\n",
       " 'flash',\n",
       " 'first',\n",
       " 'place',\n",
       " 'played',\n",
       " 'Yuumi',\n",
       " 'yesterday',\n",
       " 'ignite',\n",
       " 'usually',\n",
       " 'on.',\n",
       " 'Ahh.',\n",
       " 'Well.',\n",
       " 'Yone',\n",
       " 'needs',\n",
       " 'to.',\n",
       " 'Hmm',\n",
       " 'okay.',\n",
       " 'Coming.',\n",
       " 'Yeah',\n",
       " 'recall.',\n",
       " 'Yeahhh.',\n",
       " \"can't\",\n",
       " 'really',\n",
       " 'step',\n",
       " 'guys.',\n",
       " 'Takes',\n",
       " 'one',\n",
       " 'knockup',\n",
       " \"I'm\",\n",
       " 'dead.',\n",
       " 'Oh',\n",
       " 'okay.',\n",
       " 'Interesting',\n",
       " 'choice',\n",
       " 'Yone.',\n",
       " 'Jesus.',\n",
       " 'Ah',\n",
       " 'even',\n",
       " 'see',\n",
       " 'Poppy.',\n",
       " \"can't\",\n",
       " 'combo',\n",
       " 'this.',\n",
       " 'Fuck.',\n",
       " \"They're\",\n",
       " 'really',\n",
       " 'tanky',\n",
       " 'see',\n",
       " 'Poppy',\n",
       " 'come.',\n",
       " 'bad',\n",
       " 'man.',\n",
       " 'Ahhh.',\n",
       " 'Yasuo',\n",
       " 'bot',\n",
       " 'really',\n",
       " 'annoying',\n",
       " \"can't\",\n",
       " 'really',\n",
       " 'anything',\n",
       " 'aggressive.',\n",
       " 'wanna',\n",
       " 'swap?',\n",
       " \"can't\",\n",
       " 'farm',\n",
       " 'top',\n",
       " 'lane',\n",
       " 'anyway.',\n",
       " 'Uhm',\n",
       " 'yeah',\n",
       " \"I'll\",\n",
       " 'catch',\n",
       " 'wave',\n",
       " 'go.',\n",
       " 'Hm',\n",
       " 'kay.',\n",
       " 'Well',\n",
       " 'mean',\n",
       " 'asked',\n",
       " 'so.',\n",
       " 'happened',\n",
       " 'Uhm',\n",
       " 'Gragas',\n",
       " 'told',\n",
       " 'Yone',\n",
       " 'go',\n",
       " 'AFK',\n",
       " 'left.',\n",
       " 'Nice.',\n",
       " 'Yeah.',\n",
       " 'fuck.',\n",
       " 'Okay.',\n",
       " 'Jesus.',\n",
       " 'Nice.',\n",
       " 'Fiora',\n",
       " 'gonna',\n",
       " 'come',\n",
       " 'probably.',\n",
       " 'Yeah.',\n",
       " 'think',\n",
       " 'think',\n",
       " 'win',\n",
       " '4v5.',\n",
       " 'Maybe',\n",
       " 'Dragon',\n",
       " 'here.',\n",
       " 'just-',\n",
       " 'honestly',\n",
       " 'think',\n",
       " 'drake.',\n",
       " 'Okay',\n",
       " \"can't\",\n",
       " 'anymore.',\n",
       " 'look',\n",
       " 'fight',\n",
       " 'cause',\n",
       " 'uhm.',\n",
       " 'guy',\n",
       " 'ult',\n",
       " 'already.',\n",
       " 'run',\n",
       " 'can.',\n",
       " 'Okay.',\n",
       " 'Nice.',\n",
       " 'No.',\n",
       " 'dies.',\n",
       " 'dead.',\n",
       " 'Yep.',\n",
       " 'Nice.',\n",
       " 'Well',\n",
       " 'Yone',\n",
       " 'AFK-ed',\n",
       " 'uh',\n",
       " 'FF',\n",
       " 'option',\n",
       " 'win',\n",
       " 'winnable.',\n",
       " 'Sweats',\n",
       " 'Drag',\n",
       " 'behind.',\n",
       " \"That's\",\n",
       " 'best',\n",
       " 'thing',\n",
       " 'Ah',\n",
       " 'Blitzcrank',\n",
       " 'Blitzcrank',\n",
       " 'steal.',\n",
       " 'Ah',\n",
       " 'flashed.',\n",
       " 'Yea',\n",
       " \"that's\",\n",
       " 'fine,',\n",
       " 'TP',\n",
       " 'coming',\n",
       " 'anyway',\n",
       " 'uh.',\n",
       " 'Stealable',\n",
       " 'well.',\n",
       " 'Oh.',\n",
       " 'stun',\n",
       " 'me.',\n",
       " 'Oh',\n",
       " 'ate',\n",
       " 'E.',\n",
       " 'Oh',\n",
       " 'fuck.',\n",
       " 'Yasuo',\n",
       " 'exhaust.',\n",
       " 'think',\n",
       " 'gonna',\n",
       " 'make',\n",
       " 'much',\n",
       " 'difference',\n",
       " 'Nice.',\n",
       " 'Okay',\n",
       " \"we're\",\n",
       " 'back',\n",
       " 'in.',\n",
       " 'Okay',\n",
       " \"I'm\",\n",
       " 'back',\n",
       " 'out.',\n",
       " 'Ah.',\n",
       " 'Gragas',\n",
       " 'say',\n",
       " 'it.',\n",
       " 'Okay',\n",
       " \"I'm\",\n",
       " 'slowly',\n",
       " 'coming.',\n",
       " 'Yeah',\n",
       " 'yeah',\n",
       " 'fight.',\n",
       " 'hate',\n",
       " 'Poppy.',\n",
       " 'Ah',\n",
       " \"that's\",\n",
       " 'annoying.',\n",
       " 'Nice',\n",
       " 'get',\n",
       " 'nice',\n",
       " 'nice',\n",
       " 'nice',\n",
       " 'nice',\n",
       " 'triple,',\n",
       " 'quadra?',\n",
       " 'Mmm',\n",
       " 'nah',\n",
       " \"I'm\",\n",
       " 'die',\n",
       " 'bitch.',\n",
       " 'FeelsBadMan.',\n",
       " \"That's\",\n",
       " 'unfortunate.',\n",
       " 'Hey',\n",
       " 'looked',\n",
       " 'cool.',\n",
       " 'Jesus',\n",
       " 'farm',\n",
       " 'fuck?',\n",
       " 'Okay',\n",
       " 'Gragas',\n",
       " 'ah',\n",
       " 'damn',\n",
       " 'oh',\n",
       " 'wait',\n",
       " 'oh.',\n",
       " 'What.',\n",
       " 'Okay.',\n",
       " 'Uhh',\n",
       " 'think',\n",
       " 'want',\n",
       " 'that.',\n",
       " 'think',\n",
       " 'wanna',\n",
       " 'push',\n",
       " 'mid.',\n",
       " 'Cause',\n",
       " 'despawns',\n",
       " '20',\n",
       " 'seconds.',\n",
       " 'takes',\n",
       " 'that.',\n",
       " 'Okay',\n",
       " 'Rakan',\n",
       " 'around',\n",
       " 'corner',\n",
       " 'waiting',\n",
       " 'yep',\n",
       " 'is.',\n",
       " 'Oh',\n",
       " 'fuck',\n",
       " 'actually',\n",
       " 'flashed',\n",
       " 'them.',\n",
       " 'Yea',\n",
       " \"we're\",\n",
       " 'pretty',\n",
       " 'far',\n",
       " 'without',\n",
       " 'vision.',\n",
       " \"There's\",\n",
       " 'much',\n",
       " \"we're\",\n",
       " '4',\n",
       " 'v',\n",
       " '5',\n",
       " 'time',\n",
       " 'Fiora',\n",
       " 'split',\n",
       " 'pushes',\n",
       " 'us.',\n",
       " 'Ah',\n",
       " \"they're\",\n",
       " 'Baron.',\n",
       " 'Wait',\n",
       " \"they're\",\n",
       " 'not.',\n",
       " 'would',\n",
       " 'not.',\n",
       " 'Oh',\n",
       " \"they're\",\n",
       " 'running',\n",
       " 'it.',\n",
       " 'Fiora',\n",
       " 'bottom',\n",
       " 'probably',\n",
       " 'TP.',\n",
       " 'Yeah',\n",
       " 'think',\n",
       " 'need',\n",
       " 'think',\n",
       " 'Yas',\n",
       " 'shieldbow',\n",
       " 'it.',\n",
       " 'try',\n",
       " 'get',\n",
       " 'Drake',\n",
       " 'Baron?',\n",
       " 'Uhh',\n",
       " \"they'll\",\n",
       " 'probably',\n",
       " 'take',\n",
       " 'Dragon',\n",
       " 'run',\n",
       " 'it.',\n",
       " \"They've\",\n",
       " 'taken',\n",
       " 'long',\n",
       " 'can.',\n",
       " 'guess',\n",
       " 'gave',\n",
       " 'oh',\n",
       " 'go',\n",
       " '.Okay',\n",
       " \"they're\",\n",
       " 'running.',\n",
       " 'Well',\n",
       " \"they're\",\n",
       " 'Dragon',\n",
       " \"that's\",\n",
       " 'good',\n",
       " 'news.',\n",
       " \"aren't?\",\n",
       " 'bad',\n",
       " 'news',\n",
       " 'neither',\n",
       " 'we.',\n",
       " 'pulled',\n",
       " 'out.',\n",
       " \"I'm\",\n",
       " 'getting',\n",
       " 'turrets',\n",
       " 'free.',\n",
       " \"Can't\",\n",
       " 'fight',\n",
       " 'anyway',\n",
       " '4',\n",
       " 'v',\n",
       " '5.',\n",
       " 'Yeah.',\n",
       " 'Oof,',\n",
       " 'beat',\n",
       " 'us',\n",
       " '5',\n",
       " 'v',\n",
       " '5.',\n",
       " 'Oh',\n",
       " 'sorry',\n",
       " '4',\n",
       " 'v',\n",
       " '5',\n",
       " 'bad',\n",
       " 'bad.',\n",
       " \"They're\",\n",
       " 'far',\n",
       " 'ahead',\n",
       " 'point',\n",
       " 'unfortunately.',\n",
       " 'Oh',\n",
       " 'fucking',\n",
       " 'Poppy',\n",
       " 'dude',\n",
       " 'hate',\n",
       " 'champ',\n",
       " \"can't\",\n",
       " 'use',\n",
       " 'fucking',\n",
       " 'spells.',\n",
       " 'Fuck',\n",
       " 'cancelled',\n",
       " 'recall.',\n",
       " 'Okay',\n",
       " 'man',\n",
       " 'Ahh.',\n",
       " \"That's\",\n",
       " 'FeelsBadMan.',\n",
       " 'respect',\n",
       " 'respect',\n",
       " 'Yone',\n",
       " 'though,',\n",
       " 'someone',\n",
       " 'flamed',\n",
       " 'straight',\n",
       " 'went',\n",
       " 'AFK.',\n",
       " 'Someone',\n",
       " 'said',\n",
       " 'quit',\n",
       " 'quit.',\n",
       " \"can't\",\n",
       " 'get.',\n",
       " 'mean',\n",
       " 'gotta',\n",
       " 'respect',\n",
       " 'it.',\n",
       " 'True.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from nltk.corpus import stopwords\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "filtered_list = []\n",
    "# \n",
    "#cleaned_word_list\n",
    "for word in cleaned_words :\n",
    "    if word.casefold() not in stop_words: \n",
    "        filtered_list.append(word)\n",
    "        \n",
    "print(\"total words after removing stopwords: \", len(filtered_list))        \n",
    "filtered_list   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ffb02d-7f28-4bdd-97ca-d94269b5e7be",
   "metadata": {},
   "source": [
    "# Creating Frequency Distributions and Analysing Sentiment using NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ac4b62-8e21-4817-a555-102e94ef5142",
   "metadata": {},
   "source": [
    "## Getting Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c39a8b1-e930-45af-adae-732d8d4093ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get frequence distribution of words\n",
    "frequency_distribution_of_filtered_list = nltk.FreqDist(filtered_list)\n",
    "frequency_distribution_of_filtered_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d97a52-df9a-4244-aab5-70f99da9132c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find most common words\n",
    "# number in bracket indicates the top x number (eg. 5 means the top 5 most frequent words)\n",
    "frequency_distribution_of_filtered_list.most_common(10)\n",
    "\n",
    "# visualise the distribution in a table\n",
    "frequency_distribution_of_filtered_list.tabulate(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec3c1e7-c681-494c-8f7a-f5ceb588df75",
   "metadata": {},
   "source": [
    "## Extracting Concordance and Collocations\n",
    "In the context of NLP, a concordance is a collection of word locations along with their context. You can use concordances to find:\n",
    "\n",
    "    How many times a word appears\n",
    "    Where each occurrence appears\n",
    "    What words surround each occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadb31bf-16ae-4425-9053-0a256f1f8f7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# find what the context surrounding a word is\n",
    "full_text_including_stopwords_and_punctuations = nltk.Text(cleaned_word_list)\n",
    "\n",
    "# set word of interest in \"\" in brackets\n",
    "full_text_including_stopwords_and_punctuations.concordance(\"go\", lines = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ebc5ce-9741-4e78-b8db-e88b61dc74c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding collocations (sequences)\n",
    "\n",
    "#step 1 define the number of ngrams the finder is looking for\n",
    "bigram_finder = nltk.collocations.BigramCollocationFinder.from_words(cleaned_word_list)\n",
    "trigram_finder = nltk.collocations.TrigramCollocationFinder.from_words(cleaned_word_list)\n",
    "\n",
    "# find the top 5 (the number in the bracket) most common bi grams\n",
    "print(\"top 10 most common bigrams: \")\n",
    "print(bigram_finder.ngram_fd.most_common(10))\n",
    "print(\"\")\n",
    "\n",
    "print(\"top 10 most common trigrams: \")\n",
    "print(trigram_finder.ngram_fd.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170c9318-b936-47d0-af3d-b28d9e69652c",
   "metadata": {},
   "source": [
    "## trying to do sentiment analysis with built in nltk model VADER \n",
    "-note: model is best suited for short texts like tweets and social media things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc082c67-6d0c-49b2-94d5-54855bc708e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# create a string of words \n",
    "string_of_cleaned_word_list = \" \".join(cleaned_word_list)\n",
    "# string_of_cleaned_word_list\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sia.polarity_scores(string_of_cleaned_word_list)\n",
    "\n",
    "# team 6 (low cohesion, mean = 2.67, 3 person team, lost) results: {'neg': 0.121, 'neu': 0.655, 'pos': 0.224, 'compound': 0.9999}\n",
    "# team 32 (high cohesion, mean = 6.83, 3 person team, won ) results: {'neg': 0.141, 'neu': 0.666, 'pos': 0.193, 'compound': 0.9996}\n",
    "# team 46 (mid cohesion, mean = 4.44, 3 person team, lost) results: {'neg': 0.152, 'neu': 0.687, 'pos': 0.161, 'compound': -0.7709}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853dc8d8-bcd1-43ed-95da-de718e461b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that combines all the steps to get sentiment \n",
    "#def get_sentiment_analysis(csv_of_transcript):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfeb46db-0dae-4f01-8fb5-f4ceff98d26e",
   "metadata": {},
   "source": [
    "## to-do: \n",
    "    \n",
    "    all the word frequency calculations\n",
    "    maybe a word cloud\n",
    "    some prelimenary sentiment analysis?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aec25c3e-c58f-411e-a6b7-30d0e55d3899",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Look at the most common top words --> add them to the stop word list\n",
    "from collections import Counter\n",
    "\n",
    "# Let's aggregate this list and identify the most common words along with how many routines they occur in\n",
    "Counter(cleaned_word_list).most_common()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d1c014-7f89-4d30-9760-ef91996f1a2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
